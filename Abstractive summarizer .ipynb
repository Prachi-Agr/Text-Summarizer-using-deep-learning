{"cells":[{"metadata":{},"cell_type":"markdown","source":"Text summarization is the process of automatically generating natural language summaries from an input\ndocument while retaining the important points. It would help in easy and fast retrieval of information.\n\nThere are two prominent types of summarization algorithms.\n\n• Extractive summarization systems form summaries by copying parts of the source text\nthrough some measure of importance and then combine those part/sentences together to\nrender a summary. \n\n• Abstractive summarization systems generate new phrases, possibly rephrasing or using\nwords that were not in the original text.For\nperfect abstractive summary, the model has to first truly understand the document and then\ntry to express that understanding in short possibly using new words and phrases. \n\n\n"},{"metadata":{},"cell_type":"markdown","source":" ## Model architecture ##\n1. Glove embedding \n2. seq2seq encoder decoder architecture with LSTM layer.\n\n\n![Image of encoder-decoder architecture ](https://cdn-images-1.medium.com/max/2560/1*nYptRUTtVd9xUjwL-cVL3Q.png)\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nimport numpy as np \nimport pandas as pd \n\nimport os\nprint(os.listdir(\"../input\"))\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction import stop_words\nimport re\nimport string\n\nimport os, sys, tarfile\nfrom sklearn.model_selection import train_test_split\n\nfrom IPython.display import Image\n\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.text import Tokenizer,text_to_word_sequence \nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input,Dense, Activation, concatenate, Embedding, Flatten, CuDNNLSTM, Bidirectional, Concatenate\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras import backend as K\nfrom keras import optimizers\n","execution_count":1,"outputs":[{"output_type":"stream","text":"['kindle-reviews', 'glove-global-vectors-for-word-representation']\n","name":"stdout"},{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"Using kindle reviews dataset."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"kindle_reviews = pd.read_csv('../input/kindle-reviews/kindle_reviews.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kindle_reviews.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"kindle_reviews.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reformatting the dataframe display\n\npd.set_option('display.max_info_columns',1000)\npd.set_option('display.max_colwidth',5000)\nkindle_reviews.drop(columns = ['asin', 'helpful', 'overall', 'reviewTime', 'reviewerID', 'reviewerName', 'unixReviewTime'],\n                    axis=1, inplace=True)\nkindle_reviews.columns = ['index','review', 'summary']\nkindle_reviews.set_index(keys = 'index',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# All the model hyperparameters are defined here, considering appropriate percentile of review and summary lengths \n\nRNN_VOCAB = 10000                           # most frequent 15K words form the vocab \nMAX_SEQUENCE_REVIEW_LENGTH = 22             \nMAX_SEQUENCE_SUMMARY_LENGTH = 9\nEMBEDDING_DIM = 200                         \nEMBEDDING_FILE_PATH = \"../input/glove-global-vectors-for-word-representation/glove.6B.\" + str(EMBEDDING_DIM) + \"d.txt\"\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text preprocessing ##\n1. lowercasing\n2. stop words removal : a new dictionary of stop words is created to preserve the meaning of the original text\n3. contractions \n4. punctuations removal\n5. Unnecessary white space \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\ncontractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"i'd\": \"i would\",\n\"i'll\": \"i will\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'll\": \"it will\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"needn't\": \"need not\",\n\"oughtn't\": \"ought not\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"she'd\": \"she would\",\n\"she'll\": \"she will\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"that'd\": \"that would\",\n\"that's\": \"that is\",\n\"there'd\": \"there had\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'll\": \"they will\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'll\": \"we will\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"who'll\": \"who will\",\n\"who's\": \"who is\",\n\"won't\": \"will not\",\n\"wouldn't\": \"would not\",\n\"you'd\": \"you would\",\n\"you'll\": \"you will\",\n\"you're\": \"you are\"\n}\n\n\ndef clean_text(text,remove_stopwords = True, max_len = 20):\n    '''\n    Given a text this function removes the punctuations, selected stopwords(because not, none convey some meaning and\n    removing these stop words changes the meaning of the sentence.) and returns the length of the remaining text string\n    '''\n    refined_stop_words = {}\n    if(remove_stopwords == True):\n        refined_stop_words = stop_words.ENGLISH_STOP_WORDS-{ \"not\",\"none\",\"nothing\",\"nowhere\",\"never\",\n                                                        \"cannot\",\"cant\",\"couldnt\",\"except\",\"hasnt\",\n                                                        \"neither\",\"no\",\"nobody\",\"nor\",\"without\"\n                                                           }\n    try:\n        #convert to lower case and strip regex\n        new_text = []\n        text = text.lower()\n        count = 0\n        for word in text.split():\n            if word in refined_stop_words:\n                continue\n            count += 1\n            if word in contractions: \n                new_text = new_text + [contractions[word]]\n            else: \n                new_text = new_text + [word]\n        new_text = new_text[0:max_len] if count>max_len else new_text\n        text = ' '.join(new_text)\n        regex = re.compile('[' + re.escape(string.punctuation) + '\\\\r\\\\t\\\\n]')\n        text = regex.sub(\" \", text)\n        text = re.sub('\\s+', ' ', text).strip()\n        text = '<start> ' + text + ' <end>'\n        return text\n    except:\n        return \"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kindle_reviews['summary'] = kindle_reviews.summary.apply(lambda x: clean_text(x, True, MAX_SEQUENCE_SUMMARY_LENGTH-2))\nkindle_reviews['review'] = kindle_reviews.review.apply(lambda x: clean_text(x, True, MAX_SEQUENCE_REVIEW_LENGTH-2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kindle_reviews.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kindle_reviews.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kindle_reviews['summary_length'] = kindle_reviews.summary.apply(lambda x: len(x.split()))\nkindle_reviews['review_length'] = kindle_reviews.review.apply(lambda x: len(x.split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(kindle_reviews.review_length.describe(percentiles = [0.25, 0.9, 0.95, 0.99]))\nkindle_reviews.review_length.hist(bins = 100,figsize = (20,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(kindle_reviews.summary_length.describe(percentiles = [0.9,0.95,0.99]))\nkindle_reviews.summary_length.hist(bins = 100,figsize = (20,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define Input output to the model ## "},{"metadata":{},"cell_type":"markdown","source":"Tokenizing this data using keras tokenizer API "},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenize = Tokenizer(num_words = RNN_VOCAB, oov_token='OOV', filters = '')\ntokenize.fit_on_texts(np.hstack([kindle_reviews['summary'],kindle_reviews['review']]))\nkindle_reviews['sequence_summary'] = tokenize.texts_to_sequences(kindle_reviews['summary'])\nkindle_reviews['sequence_review'] = tokenize.texts_to_sequences(kindle_reviews['review'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total number of unique words = \", len(tokenize.word_index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The decoder input, should have a start and end token, and thus we add it to our vocab"},{"metadata":{},"cell_type":"markdown","source":"Every encoder, decoder input - summary is augmented with '<start\\>' and preceeded by '<end\\>' "},{"metadata":{"trusted":true},"cell_type":"code","source":"#craeting dataset for the model input output\ndataset = {}\ndataset['decoder_input'] = pad_sequences(kindle_reviews.sequence_summary, maxlen = MAX_SEQUENCE_SUMMARY_LENGTH, padding='post')\nkindle_reviews['sequence_summary'] = kindle_reviews.sequence_summary.apply(lambda x: x[1:])\ndataset['decoder_output'] = pad_sequences(kindle_reviews.sequence_summary, maxlen = MAX_SEQUENCE_SUMMARY_LENGTH-1, padding='post')\ndataset['encoder_input'] = pad_sequences(kindle_reviews.sequence_review, maxlen = MAX_SEQUENCE_REVIEW_LENGTH, padding='pre')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['decoder_output'].shape, dataset['decoder_input'].shape, dataset['encoder_input'].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['encoder_input'][0:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kindle_val_reviews = kindle_reviews.loc[1000:1005,:].copy()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Embeddings ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Embeddings:\n    \"\"\"\n    When a corpus is passed, remove the words which are not in the global vocab(glove) and use most frequent vocab_size\n    number of words. \n    \"\"\"\n    def __init__(self, embedding_dim, vocab_size):\n        self.embedding_dim = embedding_dim\n        self.vocab_size = vocab_size\n        \n    def readEmbeddings(self, filePath):\n        \"\"\"\n        Given a filepath of word embeddings creates and returns a dictionary of word, embedding values\n        \"\"\"\n        # Create a dictionary for storing all {word, embedding values}\n        wordToEmbeddingDict = {}\n        # open the file as read only\n        file = open(filePath, encoding='utf-8')\n        # read all text\n        for line in file:\n            lineValue = line.split()\n            word = lineValue[0]\n            embedding = np.asarray(lineValue[1:],dtype = 'float32')\n            wordToEmbeddingDict[word] = embedding\n        # close the file\n        file.close()\n        return wordToEmbeddingDict\n    \n    def indexToEmbedding(self, wordToIndexDict, wordToEmbeddingDict):\n        indexToEmbeddingMatrix = np.zeros((self.vocab_size, self.embedding_dim))\n        for word, index in wordToIndexDict.items():\n            if index >= self.vocab_size:\n                break\n            if word in wordToEmbeddingDict.keys():\n                indexToEmbeddingMatrix[index] = wordToEmbeddingDict[word]\n            else:\n                indexToEmbeddingMatrix[index] = np.array(np.random.uniform(-1.0, 1.0, self.embedding_dim))\n        return indexToEmbeddingMatrix\n    \n    def indexToWord(self, wordToIndexDict):\n        return {index: word for word, index in wordToIndexDict.items()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings = Embeddings(EMBEDDING_DIM, RNN_VOCAB)\nwordToEmbeddingDict = embeddings.readEmbeddings(EMBEDDING_FILE_PATH)\n\nindexToEmbeddingMatrix = embeddings.indexToEmbedding(tokenize.word_index, wordToEmbeddingDict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indexToWordDict = embeddings.indexToWord(tokenize.word_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indexToEmbeddingMatrix.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define the model ##\n\nModel outputs probability of each word in the vocab (10K) probabilities, for each timestep.\nThe number of timesteps for decoder are 9. We create batches of the input and output as the model trains and the previous batch is discarded at every step of the epoch.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 2096\nNUM_EPOCHS = 3\nSTEPS_PER_EPOCH = 150\nLATENT_DIM = 512                           # Dimensions of LSTM output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_batch_data(dataset, start, end):\n    # Decoder output will be one hot encoded values \n    # dimensions of the decoder output will be (number of samples * summary length * vocab size)\n    assert start < end\n    assert end <= dataset['encoder_input'].shape[0]\n    encoder_batch_input = dataset['encoder_input'][start:end]\n    decoder_batch_input = dataset['decoder_input'][start:end]\n    decoder_batch_output = np.zeros(((end-start), MAX_SEQUENCE_SUMMARY_LENGTH, RNN_VOCAB), dtype = 'float16')\n    for k, row in enumerate(dataset['decoder_output'][start:end]):\n        for i,value in enumerate(row):\n            if value!=0:\n                decoder_batch_output[k, i, value] = 1\n    return encoder_batch_input, decoder_batch_input, decoder_batch_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This generate method loops indefinitely on our dataset to create training batches\ndef generate_batch_data(dataset):\n    size = dataset['encoder_input'].shape[0]\n    while True:\n        start = 0\n        end = start+BATCH_SIZE\n        while True:\n            # create numpy arrays of input data\n            # and labels, from each line in the file\n            if start>=size:\n                break\n            encoder_batch_input, encoder_batch_output, decoder_batch_output = get_batch_data(dataset, start, end)\n            start = end\n            end = np.min([end+BATCH_SIZE, size])\n            yield ({'review': encoder_batch_input, \n                    'summary': encoder_batch_output},\n                   {'decoder_dense_layer': decoder_batch_output})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review_input_layer = Input(batch_shape=(BATCH_SIZE, MAX_SEQUENCE_REVIEW_LENGTH, ), name = 'review')\nembedding_encoder_layer = Embedding(input_length = MAX_SEQUENCE_REVIEW_LENGTH,\n                          input_dim = RNN_VOCAB,\n                          output_dim = EMBEDDING_DIM,\n                          weights=[indexToEmbeddingMatrix],\n                          trainable = False,\n                          name = 'embedding_encoder',\n                          )\nembedding_review_output = embedding_encoder_layer(review_input_layer)\nencoder_lstm_layer = Bidirectional(CuDNNLSTM(LATENT_DIM, return_state=True, name = 'lstm_encoder', stateful = True), merge_mode = 'concat')\n_, forward_h, forward_c, backward_h, backward_c = encoder_lstm_layer(embedding_review_output)\nstate_h = Concatenate()([forward_h, backward_h])\nstate_c = Concatenate()([forward_c, backward_c])\nencoder_states = [state_h, state_c]\n\n\nsummary_input_layer = Input(batch_shape=(BATCH_SIZE ,MAX_SEQUENCE_SUMMARY_LENGTH, ), name = 'summary')\nembedding_decoder_layer = Embedding(#input_length = MAX_SEQUENCE_SUMMARY_LENGTH,\n                          input_dim = RNN_VOCAB,\n                          output_dim = EMBEDDING_DIM,\n                          weights=[indexToEmbeddingMatrix],\n                          trainable=False,\n                          name = 'embedding_decoder',\n                          )\nembedding_summary_output = embedding_decoder_layer(summary_input_layer)\ndecoder_lstm_layer = CuDNNLSTM(2*LATENT_DIM, return_state=True, return_sequences = True, name = 'lstm_decoder', stateful = True)\ndecoder_output, decoder_h, decoder_c = decoder_lstm_layer(embedding_summary_output,\n                                                   initial_state = encoder_states)\n\ndecoder_dense_layer = Dense(RNN_VOCAB, activation=\"softmax\", name='decoder_dense_layer')\ndecoder_dense_output =  decoder_dense_layer(decoder_output)\n\nmodel = Model([review_input_layer, summary_input_layer], decoder_dense_output) \nsgd = optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=None, decay=0.0)\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(model, to_file='model.png', show_shapes=True)\nImage(filename='model.png') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit_generator(generator = generate_batch_data(dataset),use_multiprocessing=True,\n                    epochs=1, steps_per_epoch = dataset['encoder_input'].shape[0]//BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Always save your weights\nmodel.save_weights('summarization_weights.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('summarization_weights.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the inference model, we need to keep in mind following few things\n\nFirst we will encode the sequence, the encoder's output is of no use thus it will be discarded, state_c and state_h of the last timestep will be used as initial state of decoder, same as training. \n\nAlso decoder will receive one input of initial time step ('<start\\>' token to get started)\n\nDecoder inputs : [encoder states , <start\\>]\n\n1) decoder RNN will output one word at a time,(output dim :{ 1, 1, RNN_VOCAB}\n\n2) from argmax index to word mapping could be used to find the output word,\n\nFor the next timestep we need the decoder's states as well as an output of decoder LSTM\n\n3) Pprovide decoder with the decoder states and the output word again\n\nWe need to encapsulate decoder in separate block otherwise it will run recursively"},{"metadata":{},"cell_type":"markdown","source":"## Inference model\nLike training we have 2 separate models for encoder and decoder. \n### 2.1 Encoder\nFrom encoder we extract the thought vector to feed to the decoder."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define inference model\nencoder_inference_model = Model(review_input_layer, encoder_states)\nplot_model(encoder_inference_model, to_file='inference_encoder.png', show_shapes=True)\n\nImage(filename='inference_encoder.png') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decoder\n- 3 inputs to the decoder are, previous time step hidden state, previous time step cell state and input at current timestep\n- For first time step we have though vector as, hidden state and cell state as inputs from previous time step\n- After that, decoder cell is called recursively"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Exactly same decoder model is used with different input and output adjustments\n\ndecoder_state_input_h = Input(shape=(2*LATENT_DIM,))  # These states are required for feeding back to our next timestep decoder\ndecoder_state_input_c = Input(shape=(2*LATENT_DIM,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\n#Now we shall reuse our decoder\nsummary_for_decoder = Input(shape=(1,))\nembedding_summary_decoder = embedding_decoder_layer(summary_for_decoder)\n\ndecoder_inference_output, decoder_states_c, decoder_states_h = decoder_lstm_layer(embedding_summary_decoder, initial_state = decoder_states_inputs)\ndecoder_states_outputs = [decoder_states_c, decoder_states_h]\noutput_prob = decoder_dense_layer(decoder_inference_output)\ndecoder_inference_model = Model(\n    [summary_for_decoder] + decoder_states_inputs,\n    decoder_states_outputs + [output_prob])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(decoder_inference_model, to_file='inference_decoder.png', show_shapes=True)\n\nImage(filename='inference_decoder.png') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_sequence(input_sequence):\n    # Encode the input as state vectors. states_h and states_c for decoder init\n    encoder_states_value = encoder_inference_model.predict(input_sequence)\n    \n    # Generate empty target sequence of length 1, for decoder input  \n    # Populate the first character of target sequence with the start character.\n    target_sequence = np.zeros((1,1))\n    target_sequence[0,0] = tokenize.word_index['start']\n    # Sampling loop for a batch of sequences\n    stop_condition = False\n    decoded_sentence = []\n   \n    while not stop_condition:\n      \n        h, c, output_tokens = decoder_inference_model.predict([target_sequence] + encoder_states_value,batch_size=1)\n        # Sample a token\n        \n        sampled_word_index = np.argmax(output_tokens)#np.random.choice(np.arange(10003), p=output_tokens.flatten())\n        if sampled_word_index == 0:\n            decoded_sentence += ['<pad>']\n            continue\n        sampled_word = indexToWordDict[sampled_word_index]\n        decoded_sentence += [sampled_word]\n        \n        # Exit condition: either hit max length=\n        # or find stop character.\n        if (sampled_word == 'end' or\n           len(decoded_sentence) > MAX_SEQUENCE_SUMMARY_LENGTH):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n            target_sequence[0, 0] = sampled_word_index\n        # Update states\n        encoder_states_value = [h, c]\n    return ' '.join(decoded_sentence)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"def summarize(input_seq):\n    input_seq = pad_sequences(tokenize.texts_to_sequences([clean_text(input_seq)]),MAX_SEQUENCE_REVIEW_LENGTH)\n    return decode_sequence(input_seq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summarize(\"The book was very nice, will read it again\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summarize(\"very very bad book, didn't like it much\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index,row in kindle_val_reviews.head(5).iterrows():\n    print(\"Input review : \" , row.review)\n    print(\"Expected review : \", row.summary)\n    print(\"Predicted output : \", summarize(row.review))\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}